{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency Prompting\n",
    "\n",
    "One of the more advanced techniques in prompt engineering is self-consistency, introduced by `Wang et al. (2022)`. \n",
    "\n",
    "This method seeks to improve upon the traditional greedy decoding typically used in chain-of-thought (CoT) prompting. \n",
    "\n",
    "The core concept involves sampling multiple diverse reasoning paths through few-shot CoT and leveraging these variations to determine the most consistent answer. The technique  enhances the effectiveness of CoT prompting, particularly for tasks requiring arithmetic and commonsense reasoning.\n",
    "\n",
    "## References:\n",
    "* [Wang et al. (2022)](https://arxiv.org/abs/2203.11171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fself_consistency.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve \\(984 \\times \\log(2)\\), we need to evaluate it using multiple reasoning paths and then compare the results.\n",
      "\n",
      "### Path 1: Using a Calculator\n",
      "\n",
      "1. **Find \\(\\log_{10}(2)\\):**  \n",
      "   Using a calculator, \\(\\log_{10}(2) \\approx 0.3010\\).\n",
      "\n",
      "2. **Multiply by 984:**  \n",
      "   \\(984 \\times 0.3010 = 296.184\\).\n",
      "\n",
      "### Path 2: Using Natural Logarithm\n",
      "\n",
      "1. **Convert to Natural Logarithm:**  \n",
      "   Recall that \\(\\log_{10}(x) = \\frac{\\ln(x)}{\\ln(10)}\\). Therefore, \\(\\log_{10}(2) = \\frac{\\ln(2)}{\\ln(10)}\\).\n",
      "\n",
      "2. **Find \\(\\ln(2)\\):**  \n",
      "   Using a calculator, \\(\\ln(2) \\approx 0.6931\\).\n",
      "\n",
      "3. **Find \\(\\ln(10)\\):**  \n",
      "   Using a calculator, \\(\\ln(10) \\approx 2.3026\\).\n",
      "\n",
      "4. **Calculate \\(\\log_{10}(2)\\):**  \n",
      "   \\(\\log_{10}(2) = \\frac{0.6931}{2.3026} \\approx 0.3010\\).\n",
      "\n",
      "5. **Multiply by 984:**  \n",
      "   \\(984 \\times 0.3010 = 296.184\\).\n",
      "\n",
      "### Path 3: Using Approximation\n",
      "\n",
      "1. **Approximate \\(\\log_{10}(2)\\):**  \n",
      "   We know \\(\\log_{10}(2) \\approx 0.3010\\) from previous calculations.\n",
      "\n",
      "2. **Multiply by 984:**  \n",
      "   \\(984 \\times 0.3010 = 296.184\\).\n",
      "\n",
      "### Comparison and Conclusion\n",
      "\n",
      "- All paths lead to the same result: \\(296.184\\).\n",
      "- The most frequently occurring answer is \\(296.184\\), which appears in all reasoning paths.\n",
      "\n",
      "Therefore, the final answer is:\n",
      "\n",
      "\\[ \\boxed{296.184} \\]\n",
      "Time taken: 6532ms\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### SELF CONSISTENCY\n",
    "###\n",
    "\n",
    "from clients import bootstrap_client_and_model\n",
    "from models import ModelOptions\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "message = \"What is 984 * log(2)\"\n",
    "\n",
    "#### (2) Instantiate ChatBot Client and preferred model\n",
    "client, model = bootstrap_client_and_model(\"phi4:latest\")\n",
    "\n",
    "#### (3) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "# TODO: PROMPT\n",
    "client.set_system_prompt(\"For every question, generate multiple reasoning paths leading to an answer. Compare the answers from all paths and select the most frequently occurring one as the final response. If there is a tie, choose the answer that appears in the most logically sound explanations. Ensure the final answer reflects the majority consensus\")\n",
    "\n",
    "#### (4) Choose Model Options\n",
    "options = ModelOptions(temperature=0.7)\n",
    "\n",
    "#### (5) Send the request to the client with the prompt, selected model, and options\n",
    "time, response = client.chat_completion(message=message,\n",
    "                             model=model,\n",
    "                             options=options)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}ms')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
