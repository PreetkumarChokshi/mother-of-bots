{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template Prompting\n",
    "\n",
    "Prompt Template Prompting refers to a technique where predefined templates are used to construct effective prompts that guide large language models (LLMs) to generate responses tailored to specific use cases. The templates typically contain static text combined with dynamic input variables, allowing for consistent, reusable, and customizable prompts.\n",
    "\n",
    "Prompt templates are widely used across various domains, such as:\n",
    "* **Question Generation**: Templates can generate quiz questions by filling in variables related to topics.\n",
    "* **Text Summarization**: Static instructions combined with variable documents or inputs allow flexible summarization.\n",
    "* **Coding Assistance**: Dynamic prompts help LLMs generate code snippets for different programming tasks.\n",
    "\n",
    "## References:\n",
    "\n",
    "* (OpenAI Documentation for Prompt Engineering)[https://platform.openai.com/docs/guides/prompt-engineering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fprompt_template.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is 984 * log(2)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#### (2) Instantiate ChatBot Client and preferred model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m client, model \u001b[38;5;241m=\u001b[39m \u001b[43mbootstrap_client_and_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphi4:latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#### (3) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# TODO : PROMPT\u001b[39;00m\n\u001b[0;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m message\n",
      "File \u001b[1;32mc:\\Users\\preet\\prompt-eng\\prompt-eng\\clients.py:290\u001b[0m, in \u001b[0;36mbootstrap_client_and_model\u001b[1;34m(preferred_model, prompt)\u001b[0m\n\u001b[0;32m    288\u001b[0m config \u001b[38;5;241m=\u001b[39m config_factory()\n\u001b[0;32m    289\u001b[0m client \u001b[38;5;241m=\u001b[39m ChatbotClientFactory\u001b[38;5;241m.\u001b[39mcreate_client(config)\n\u001b[1;32m--> 290\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preferred_model:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# If preferred model is specified, try to use it\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     picked_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((model \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m preferred_model), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\preet\\prompt-eng\\prompt-eng\\clients.py:126\u001b[0m, in \u001b[0;36mOpenWebUIClient.get_models\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_info \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    124\u001b[0m     model \u001b[38;5;241m=\u001b[39m AIModel(\n\u001b[0;32m    125\u001b[0m         name\u001b[38;5;241m=\u001b[39mmodel_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m--> 126\u001b[0m         parameter_size\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ollama'"
     ]
    }
   ],
   "source": [
    "##\n",
    "## PROMPT TEMPLATE PROMPTING\n",
    "##\n",
    "\n",
    "from clients import bootstrap_client_and_model\n",
    "from models import ModelOptions\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "message = \"What is 984 * log(2)\"\n",
    "\n",
    "#### (2) Instantiate ChatBot Client and preferred model\n",
    "client, model = bootstrap_client_and_model(\"phi4:latest\")\n",
    "\n",
    "#### (3) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "# TODO : PROMPT\n",
    "prompt = message\n",
    "\n",
    "#### (4) Choose Model Options\n",
    "options = ModelOptions(\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "#### (5) Send the request to the client with the prompt, selected model, and options\n",
    "time, response = client.chat_completion(message=prompt,\n",
    "                             model=model,\n",
    "                             options=options)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}ms')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
